{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Spark_Tutorial.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/nazbeh/I_C_M_E_2020/blob/master/Workshop4/Spark_Tutorial.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LqIja-mnRpto",
        "colab_type": "text"
      },
      "source": [
        "# Spark Tutorial"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t863PmpkSV8o",
        "colab_type": "text"
      },
      "source": [
        "## Setup of the VM \n",
        "First execute this cell to setup the Virtual Machine using Spark. This steps may change if running locally or in another cluster. After this you can run independently the rest of the sections"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xyQ9GxwHRmTU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Install dependencies needed for pySpark\n",
        "!sudo apt update\n",
        "!apt-get install openjdk-8-jdk-headless -qq > /dev/null\n",
        "!wget -q https://downloads.apache.org/spark/spark-3.0.0/spark-3.0.0-bin-hadoop3.2.tgz\n",
        "!tar -xvf spark-3.0.0-bin-hadoop3.2.tgz\n",
        "!pip install -q findspark\n",
        "\n",
        "#Set paths for Java and Spark\n",
        "import os\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
        "os.environ[\"SPARK_HOME\"] = \"/content/spark-3.0.0-bin-hadoop3.2\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kqvS7s1DS9qV",
        "colab_type": "text"
      },
      "source": [
        "## Initialize Spark in Python"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aX_vhwNCTDqw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Easy way to add PySpark to sys.path \n",
        "import findspark\n",
        "findspark.init()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DRln8KiHUJx_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Create a spark session\n",
        "from pyspark.sql import SparkSession\n",
        "\n",
        "spark = SparkSession.builder.master(\"local[*]\").getOrCreate()\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GrwFM-S9wT7H",
        "colab_type": "text"
      },
      "source": [
        "## Hello World"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bhhu30dTwnPF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import findspark\n",
        "findspark.init()\n",
        "from pyspark.sql import SparkSession\n",
        "\n",
        "def main():\n",
        "  #Create a spark session\n",
        "  spark = SparkSession.builder.appName(\"HelloWorld\").getOrCreate()\n",
        "  sc = spark.sparkContext\n",
        "\n",
        "  nums = sc.parallelize([0,1,2,3,4])\n",
        "  print(nums.map(lambda x: x+x).collect())\n",
        "  spark.stop()\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "  main()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DjDL_zpCxDyS",
        "colab_type": "text"
      },
      "source": [
        "## Example: Compute Pi"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IPS6gmB3xHC6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import findspark\n",
        "findspark.init()\n",
        "from operator import add\n",
        "from pyspark.sql import SparkSession\n",
        "\n",
        "spark = SparkSession.builder.appName(\"Pi\").getOrCreate()\n",
        "\n",
        "n = 100000\n",
        "\n",
        "s = spark.sparkContext.parallelize(range(n))\\\n",
        "    .map(lambda i:4./(1.+(i+0.5)**2./n**2.)*1./n)\\\n",
        "    .reduce(add)\n",
        "\n",
        "print(\"Pi is %f\\n\"%s)\n",
        "spark.stop()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rr6vvMk-q3a5",
        "colab_type": "text"
      },
      "source": [
        "## Example: Page Rank\n",
        "You can find this and more examples in https://github.com/apache/spark/tree/master/examples/src/main/python"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8N7f9wosq2qP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import findspark\n",
        "findspark.init()\n",
        "import numpy as np\n",
        "from pyspark.sql import SparkSession\n",
        "from operator import add\n",
        "\n",
        "# Creates a Spark Session\n",
        "spark = SparkSession.builder.master(\"local[*]\").getOrCreate()\n",
        "\n",
        "def compute_contribution(nodes,weight):\n",
        "  tot_node = len(nodes)\n",
        "  for node in nodes:\n",
        "    yield (node, weight / tot_node)\n",
        "\n",
        "#Create an RDD\n",
        "data = spark.sparkContext.parallelize([[0,1],[1,2],[2,3],[5,2],[5,3],[1,3],[4,5],[3,2],[3,4],[3,5],[0,2]])\n",
        "\n",
        "#Groups the data creating an adjacency matrix by key\n",
        "S = data.groupByKey()\n",
        "\n",
        "#Initializes de vector r will all ones from nodes of adjacency matrix\n",
        "r = S.map(lambda node: (node[0], 1.0))\n",
        "\n",
        "for iter in range(10):\n",
        "  ## Joins S and r, and applies the transformation compute_contribution\n",
        "  new_S = S.join(r).flatMap(lambda node: compute_contribution(node[1][0],node[1][1]))\n",
        "  ## Computes new iteration of r\n",
        "  r = new_S.reduceByKey(add)\n",
        "\n",
        "#Collects in driver\n",
        "print(r.collect())\n",
        "spark.stop()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KOa5y9ZWqoxH",
        "colab_type": "text"
      },
      "source": [
        "## Exercise: K-means \n",
        "\n",
        "Complete the following code following the ```To do``` suggestions "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Hujkbs2OlSbY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import findspark\n",
        "findspark.init()\n",
        "import numpy as np\n",
        "# To do here: Import SparkSession from pyspark\n",
        "\n",
        "from operator import add\n",
        "\n",
        "#To do here: Create a local Spark session\n",
        "spark = \n",
        "\n",
        "\n",
        "#Function to compute clusters\n",
        "def get_cluster(x,clusters):\n",
        "    dist = np.array(x)-clusters\n",
        "    dist = np.sqrt(np.sum(dist**2.,1))\n",
        "    i = np.argmin(dist)\n",
        "    return i\n",
        "\n",
        "#To do here: Create a RDD from the list\n",
        "#[[1, 2, 3], [4, 5, 6], [7, 8, 9], [10, 11, 12],[5,2,5],[3,4,5]]\n",
        "data = \n",
        "\n",
        "#Number of clusters \n",
        "K = 2\n",
        "\n",
        "#Initial clusters taken at random\n",
        "centroids = data.takeSample(False,K)\n",
        "tol = 1E-4\n",
        "error = 1.\n",
        "\n",
        "print(\"Centroids\",centroids)\n",
        "while error > tol:\n",
        "  #To do here: transform data using a *map* to create a key,value map \n",
        "  #applying the lambda function\n",
        "  #lambda x: (get_cluster(x,centroids), np.array(x))\n",
        "  assign_cluster = \n",
        "\n",
        "  #To do here: compute de sum of data per cluster using *reduceByKey*\n",
        "  sum_cluster = \n",
        "\n",
        "  #To do here: compute de number of points per cluster using *countByKey*\n",
        "  size_cluster = \n",
        "\n",
        "  #To do here: collect the sum_cluster in the driver using *collect*\n",
        "  sum_clusters = \n",
        "\n",
        "  #Computes the new centroids\n",
        "  new_centroids = []\n",
        "  for key,coord in sum_clusters:\n",
        "    new_centroids.append(coord/size_cluster[key])\n",
        "  new_centroids = np.array(new_centroids)\n",
        "\n",
        "  error = np.sqrt(np.sum((new_centroids-centroids)**2.))\n",
        "  centroids = new_centroids\n",
        "\n",
        "  print(\"Centroids\",new_centroids)\n",
        "  print(\"Convergence error\",error )\n",
        "spark.stop()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "II0FnVCCQhm1",
        "colab_type": "text"
      },
      "source": [
        "## Exercise: Correct Page Rank\n",
        "Notice that the code in the example does not completely compute the Page rank iteration (and as you may have notice in the previous notebooks neither the cpp implementations that we have use). For now we are computing the power method, i.e.\n",
        "$$ r^{k+1} = S*r^{k} $$\n",
        "whereas the Page rank algorithm is\n",
        "$$ r^{k+1} = 0.85 S*r^{k} + 0.15 * 1$$\n",
        "Modify the spark code before to add these changes."
      ]
    }
  ]
}