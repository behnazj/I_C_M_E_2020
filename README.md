# Information

This Repository contains files and lecture slides from  I_C_M_E 2020 Summer Workshops titled Fundamentals of Data Science. It covered a wide range of topics from Machine Learning to HCP. Some documents are publickly avialbe from instructors' websites (copyright).

- Workshop #0: Stat
- Workshop #1: Py
- [Workshop #2](https://sites.google.com/site/ml2020icme/home): ML
- [Workshop #3](https://icme-workshops.github.io/deep-learning/): DL
- Workshop #4: HCP

# Additional Resources

## Statistics

- Session 1
  - Descriptive statistics for exploring data, especially visualization
  - Sampling, studies and experiments
- Session 2 
  - Probability: basic rules, conditional probability, Bayes' rule
  - Sampling distributions and the central limit theorem
- Session 3
  - Regression
  - Confidence intervals and the bootstrap principle
- Session 4
  - Tests of significance, multiple comparisons and reproducibility

Here is an additional resource:

- Statistics Visualizations - [Seeing Theory](https://seeing-theory.brown.edu/)

## Machine Learning 

- Session 1 Introduction
  - Overview of Machine Learning
- Session 2 Unsupervised Learning
  - Clustering (K-means, Hierarchical Clustering)
  - Dimensionality Reduction (PCA, ICA, MDS, SOM, tSNE)
  - Imputation
- Session 3
  - Supervised Learning: Principles
    - Cross-validation
    - Regularization and Sparsity (lasso, ridge regression, elastic net)
- Session 4
  - Supervised Learning: Methods
    - Classification and Regression Trees 
    - Ensembles
    - Neural

Reference books (see reference folder):

1. [An Introduction to Statistical Learning with Applications in R](http://faculty.marshall.usc.edu/gareth-james/ISL/) by G. James, D. Witten, T. Hastie, and R. Tibshirani - Free online book
2. [The Elements of Statistical Learning](https://web.stanford.edu/~hastie/ElemStatLearn/) by T. Hastie, R. Tibshirani, and J. Friedman - Free online book

##  Deep Learning

- Session 1 
  - Introduction
  - Current state of the art in deep learning
  - Math review
  - Architecture of multi-layer neural networks
- Session 2 
  - Loss functions
  - The backpropgation algorithm
  - The gradient descent algorithm
  - Over-fitting and Under-fitting
- Session 3 
  - Convolutional Neural Networks
  - Recurrent Neural Networks
  - Other Architectures
  - Deep Learning Libraries
  - Hands-on coding Session - Tensorflow
- Session 4
  - Hands-on coding Session - Keras
  - Hands-on coding Session - Transfer Learning
  - Failures of deep learning

Here are some additional resources for various topics:

- Books
  - [Neural Networks and Deep Learning](http://neuralnetworksanddeeplearning.com/) by Michael Nielsen - Free online book
  - [Deep Learning](https://www.deeplearningbook.org/) by Ian Goodfellow and Yoshua Bengio and Aaron Courville
- Visualizations
  - [Neural Network Playground](https://playground.tensorflow.org/) - A playground for dense neural networks
  - [Gan Lab](https://poloclub.github.io/ganlab/) - A playground for GANs
  - [Initializing neural networks](https://www.deeplearning.ai/ai-notes/initialization/) - Visual tutorial on initialization in deep learning
  - [Parameter optimization in neural networks](https://www.deeplearning.ai/ai-notes/optimization/) - Visual tutorial on optimization in deep learning
- Stanford Courses
  - [CS 230 Deep Learning](https://cs230.stanford.edu/)
  - [CS 231N Convolutional Neural Networks for Visual Recognition](http://cs231n.stanford.edu/)
  - [CS 224N Natural Language Processing with Deep Learning](http://web.stanford.edu/class/cs224n/)
  - [CS 236 Deep Generative Models](https://deepgenerativemodels.github.io/)
- Interesting talks on advanced topics
  - Ben Recht - [Training on test set and other heresies](https://www.youtube.com/watch?v=NTz4rJS9BAI)
  - Aleksander Madry - [A new perspective on Adversarial Perturbations](https://www.youtube.com/watch?v=mUt7w4UoYqM)

##  High Performance Computing

- Session 1: Algorithms 
- Session 2: Shared Memory
- Session 3: Distributed Memory 
- Session 4: Spark
